Processing config: Config(model=<LocalModel.QWEN_2_5_7B_INSTRUCT: 'Qwen/Qwen2.5-7B-Instruct'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
Creating pipeline for Qwen/Qwen2.5-7B-Instruct
Loading local model: Qwen/Qwen2.5-7B-Instruct
Local model loaded and generator is ready.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/BFCL_v4_multiple_qwen2_5_7b.json: {'accuracy': 0.94, 'total_cases': 200, 'correct_cases': 188}
Completed processing for config: Config(model=<LocalModel.QWEN_2_5_7B_INSTRUCT: 'Qwen/Qwen2.5-7B-Instruct'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
Processing config: Config(model=<LocalModel.QWEN_2_5_7B_INSTRUCT: 'Qwen/Qwen2.5-7B-Instruct'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.PARAPHRASE: 3>)
File result/inference_raw/BFCL_v4_multiple_qwen2_5_7b_para.json not found. It will be created.
Reusing existing pipeline for Qwen/Qwen2.5-7B-Instruct

Processing batch 1: cases 0 to 24
Calling model interface for batch of size 24...
Generating responses for batch of 24 inputs...
Cleaning up model Qwen/Qwen2.5-7B-Instruct...
