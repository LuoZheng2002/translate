Loading configs from: config_slurm.py
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
File result/inference_raw/Qwen-Qwen3-8B/vanilla.json not found. It will be created.
Local model inference configuration:
  Model: Qwen/Qwen3-8B
  Model size: 8B
  Number of GPUs: 1
  Calculated batch size: 15 (formula: batch_size * 8 = 120 * 1)
Creating vLLM pipeline for Qwen/Qwen3-8B
INFO 11-25 01:14:54 [__init__.py:239] Automatically detected platform cuda.
Loading local model with vLLM: Qwen/Qwen3-8B
INFO 11-25 01:15:19 [config.py:717] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed', 'score'}. Defaulting to 'generate'.
INFO 11-25 01:15:20 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 11-25 01:15:25 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 11-25 01:15:26 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9e0e665c70>
INFO 11-25 01:15:28 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 11-25 01:15:28 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 11-25 01:15:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-25 01:15:28 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-8B...
INFO 11-25 01:15:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 11-25 01:16:06 [weight_utils.py:281] Time spent downloading weights for Qwen/Qwen3-8B: 37.629550 seconds
INFO 11-25 01:16:18 [loader.py:458] Loading weights took 11.69 seconds
INFO 11-25 01:16:19 [gpu_model_runner.py:1347] Model loading took 15.2683 GiB and 50.624067 seconds
INFO 11-25 01:16:34 [backends.py:420] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/a8274931ea/rank_0_0 for vLLM's torch.compile
INFO 11-25 01:16:34 [backends.py:430] Dynamo bytecode transform time: 15.62 s
INFO 11-25 01:16:43 [backends.py:136] Cache the graph of shape None for later use
INFO 11-25 01:17:27 [backends.py:148] Compiling a graph for general shape takes 52.02 s
INFO 11-25 01:18:04 [monitor.py:33] torch.compile takes 67.63 s in total
INFO 11-25 01:18:05 [kv_cache_utils.py:634] GPU KV cache size: 132,544 tokens
INFO 11-25 01:18:05 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 32.36x
INFO 11-25 01:18:31 [gpu_model_runner.py:1686] Graph capturing finished in 26 secs, took 0.60 GiB
INFO 11-25 01:18:31 [core.py:159] init engine (profile, create kv cache, warmup model) took 132.93 seconds
INFO 11-25 01:18:31 [core_client.py:439] Core engine process 0 ready.
WARNING 11-25 01:18:31 [sampling_params.py:347] temperature 0.001 is less than 0.01, which may cause numerical errors nan or inf in tensors. We have maxed it out to 0.01.
vLLM model loaded and generator is ready.

Processing batch 1: cases 0 to 15
Calling model interface for batch of size 15...
Generating responses for batch of 15 inputs with vLLM...
Cleaning up vLLM model Qwen/Qwen3-8B...
Warning: Error during cleanup of Qwen/Qwen3-8B: cannot access local variable 'llm' where it is not associated with a value
